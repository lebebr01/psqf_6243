---
title: An R Markdown document converted from "course-notes/09-multiple-regression-p1.ipynb"
output: html_document
---

# Multiple Regression

So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single $X$ attribute.

$$
Y = \beta_{0} + \beta_{1} X + \epsilon
$$

However, there are many situations where we would want more than one predictor attribute in the data. We can simply add another predictor attribute and this would look like the following regression equation. 

$$
Y = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \epsilon
$$

where $X_{1}$ and $X_{2}$ are two different predictors attributes. Let's dive right into some data to explore this a bit more. 

## Data

For this portion, using data from the college scorecard representing information about higher education institutions. 

```{r}
library(tidyverse)
library(ggformula)

theme_set(theme_bw(base_size = 18))

college <- read_csv("https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv")

head(college)
```

## Question

Suppose we were interested in exploring admission rates and which attributes helped to explain variation in admission rates. 

```{r}
gf_density(~ adm_rate, data = college) %>%
  gf_labs(x = "Admission Rate")
```

```{r}
gf_point(adm_rate ~ actcmmid, data = college) %>%
  gf_smooth(method = 'loess') %>%
  gf_labs(x = "Median ACT Score",
          y = "Admission Rate")
```

```{r}
library(GGally)

ggpairs(college[c('adm_rate', 'actcmmid', 'costt4_a')])
```

Let's now fit a multiple regression model. 

```{r}
adm_mult_reg <- lm(adm_rate ~ actcmmid + costt4_a, data = college)

summary(adm_mult_reg)
```

This model is now the following form:

$$
Admission\ Rate = 1.1 + -0.017 ACT + -0.000002 cost + \epsilon
$$

1. Why are these parameter estimates so small? 
2. How well is the overall model doing? 
3. Are both terms important in understanding variation in admission rates? How can you tell?

## Variance decomposition

Ultimately, multiple regression is a decomposition of variance. Recall, 

$$ 
\sum (Y - \bar{Y})^2 =  \sum (\hat{Y} - \bar{Y})^2 + \sum (Y - \hat{Y})^2  \\[10pt]
SS_{Total} = SS_{Regression} + SS_{Error}
$$

Multiple regression still does this, but now, there are two attributes going into helping explain variation in the regression portion of the variance decomposition. How is this variance decomposed by default? For linear regression, the variance decomposition is commonly done using type I sum of square decomposition. What does this mean? Essentially, this means that additional terms are added to determine if they help explain variation over and above the other terms in the model. This is a conditional variance added. For example, given the model above, the variance decomposition could be broken down into the following.

$$
SS_{Total} = SS_{Regression} + SS_{Error}   \\[10pt]
SS_{Total} = SS_{X_{1}} + SS_{X_{2} | X_{1}} + SS_{Error}
$$

```{r}
act_lm <- lm(adm_rate ~ actcmmid, data = college)

summary(act_lm)
```

```{r}
anova(act_lm)
anova(adm_mult_reg)
```

```{r}
cost_lm <- lm(adm_rate ~ costt4_a, data = college)

summary(cost_lm)
anova(cost_lm)
```

